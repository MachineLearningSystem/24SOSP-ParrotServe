{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4cee1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def callgpt(prompt, query):\n",
    "    url = 'https://gcrendpoint.azurewebsites.net/openai/deployments/gpt-4-32k/chat/completions?api-version=2023-03-15-preview'\n",
    "    api_key = 'oaip_qKIbzuMTnmjWfbUgSwXVFOvgZgTWmaGp'  \n",
    "    headers = {'Content-Type': 'application/json', 'api-key': api_key}  \n",
    "    data = {\n",
    "        \"messages\": [  \n",
    "            {\"role\": \"system\", \"content\": prompt},  \n",
    "            {\"role\": \"user\", \"content\": query},\n",
    "        ],\n",
    "        \"max_tokens\": 4000,\n",
    "        \"temperature\": 0.7,\n",
    "        \"n\":1\n",
    "    }\n",
    "\n",
    "    resp = requests.post(url, json=data, headers=headers)\n",
    "    return resp.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c97d89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json  \n",
    "  \n",
    "# Path to your .jsonl file  \n",
    "jsonl_file_path = 'multiagent-pytorch.json'\n",
    "  \n",
    "# Initialize an empty list to store the JSON objects  \n",
    "data = []  \n",
    "  \n",
    "# Open the file and read line by line  \n",
    "with open(jsonl_file_path, 'r', encoding='utf-8') as file:  \n",
    "    lines = file.readlines()\n",
    "    # Parse each line as a JSON object and append to the list  \n",
    "    data = json.loads('\\n'.join(lines))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecf19588",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Creating a deep learning framework like PyTorch is a massive undertaking that involves many components such as tensor operations, automatic differentiation, neural network layers, optimizers, and utilities for data loading and model saving. Below is a simplified, high-level architecture for such a framework.\\n\\nFiles:\\n- tensor.py\\n- autograd.py\\n- module.py\\n- layers.py\\n- optim.py\\n- utils.py\\n- dataloader.py\\n- main.py\\n\\nAPIs:\\ntensor.py:\\n```python\\nclass Tensor:\\n    def __init__(self, data, requires_grad=False):\\n        # Initializes the tensor with data and a boolean indicating if it requires gradient computation\\n\\n    def __add__(self, other):\\n        # Overloads the addition operator\\n\\n    def __mul__(self, other):\\n        # Overloads the multiplication operator\\n\\n    def backward(self):\\n        # Performs backpropagation starting from this tensor\\n\\n    def zero_grad(self):\\n        # Zeros the gradient of the tensor\\n```\\n\\nautograd.py:\\n```python\\nclass Function:\\n    @staticmethod\\n    def forward(ctx, *args, **kwargs):\\n        # Defines the forward pass of a function\\n\\n    @staticmethod\\n    def backward(ctx, *grad_outputs):\\n        # Defines the backward pass of a function\\n```\\n\\nmodule.py:\\n```python\\nclass Module:\\n    def __init__(self):\\n        # Initializes the module\\n\\n    def forward(self, *input):\\n        # Defines the forward pass of a module\\n\\n    def __call__(self, *input):\\n        # Allows the module to be called like a function\\n\\n    def parameters(self):\\n        # Returns an iterator over module parameters\\n\\n    def zero_grad(self):\\n        # Zeros the gradient for all parameters in the module\\n```\\n\\nlayers.py:\\n```python\\nclass Linear(Module):\\n    def __init__(self, in_features, out_features):\\n        # Initializes the linear layer with weights and bias\\n\\n    def forward(self, input):\\n        # Applies the linear transformation to the input data\\n```\\n\\noptim.py:\\n```python\\nclass Optimizer:\\n    def __init__(self, params, lr=0.001):\\n        # Initializes the optimizer with parameters and learning rate\\n\\n    def step(self):\\n        # Performs a single optimization step\\n\\n    def zero_grad(self):\\n        # Zeros the gradients of all parameters\\n```\\n\\nutils.py:\\n```python\\ndef save_model(model, file_path):\\n    # Saves the model to a file\\n\\ndef load_model(file_path):\\n    # Loads a model from a file\\n```\\n\\ndataloader.py:\\n```python\\nclass DataLoader:\\n    def __init__(self, dataset, batch_size=1, shuffle=False):\\n        # Initializes the dataloader with a dataset, batch size, and shuffle option\\n\\n    def __iter__(self):\\n        # Provides an iterator to load data in batches\\n```\\n\\nmain.py:\\n```python\\ndef main():\\n    # Entry point for the deep learning framework\\n    # Example: Training a model, testing, etc.\\n\\nif __name__ == \"__main__\":\\n    main()\\n```\\n\\nThis is a very basic blueprint and does not include many features such as device management (CPU/GPU), advanced layers, loss functions, or more complex optimizers. Each of these files would contain multiple classes and functions, and building out the full functionality would take a substantial amount of code. Moreover, real-world deep learning frameworks are optimized for performance with underlying C/C++/CUDA implementations.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[2]['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35caf640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "```python\n",
      "class Tensor:\n",
      "    def __init__(self, data, requires_grad=False):\n",
      "        # Initializes the tensor with data and a boolean indicating if it requires gradient computation\n",
      "\n",
      "    def __add__(self, other):\n",
      "        # Overloads the addition operator\n",
      "\n",
      "    def __mul__(self, other):\n",
      "        # Overloads the multiplication operator\n",
      "\n",
      "    def backward(self):\n",
      "        # Performs backpropagation starting from this tensor\n",
      "\n",
      "    def zero_grad(self):\n",
      "        # Zeros the gradient of the tensor\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "file_apis = {}\n",
    "parsing_code = False\n",
    "code = \"\"\n",
    "for line in data[2]['content'].split('\\n'):\n",
    "    if '.py' in line:\n",
    "        if '-' in line:\n",
    "            continue\n",
    "        filename = line.split(\":\")[0]\n",
    "        parsing_code = True\n",
    "        continue\n",
    "    if parsing_code:\n",
    "        code = code + '\\n' + line\n",
    "        if line == '```':\n",
    "            file_apis[filename] = code\n",
    "            parsing_code = False\n",
    "print(file_apis['tensor.py'])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6caef57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jinja2 import Template\n",
    "def round1(file_apis):\n",
    "    system = \"\"\"\n",
    "NOTICE\n",
    "Role: You are a professional engineer; the main goal is to write PEP8 compliant, elegant, modular, easy to read and maintain Python 3.9 code (but you can also use other programming language)\n",
    "ATTENTION: Use '##' to SPLIT SECTIONS, not '#'. Output format carefully referenced \"Format example\".\n",
    "\n",
    "## Code: Write code with triple quoto, based on the following list and context.\n",
    "1. Do your best to implement THIS ONLY ONE FILE. ONLY USE EXISTING API. IF NO API, IMPLEMENT IT.\n",
    "2. Requirement: Based on the context, implement one following code file, note to return only in code form, your code will be part of the entire project, so please implement complete, reliable, reusable code snippets\n",
    "3. Attention1: If there is any setting, ALWAYS SET A DEFAULT VALUE, ALWAYS USE STRONG TYPE AND EXPLICIT VARIABLE.\n",
    "4. Attention2: YOU MUST FOLLOW \"Data structures and interface definitions\". DONT CHANGE ANY DESIGN.\n",
    "5. Think before writing: What should be implemented and provided in this document?\n",
    "6. CAREFULLY CHECK THAT YOU DONT MISS ANY NECESSARY CLASS/FUNCTION IN THIS FILE.\n",
    "7. Do not use public member functions that do not exist in your design.\n",
    "\"\"\"\n",
    "    user_template = Template(\"\"\"\n",
    "    Files:\n",
    "    {{filenames}}\n",
    "\n",
    "    APIs:\n",
    "    {{apis}}\n",
    "\n",
    "    You only need to implement {{implement}}. Implement all functions and additional functions you need. DO NOT LET ME TO IMPLEMENT ANYTHING!!!!\n",
    "    Make sure your response code is runnable.\n",
    "    Do not response any content in {{otherfiles}}. Strictly follow the response format. Do not answer any other content or suggestions.\n",
    "    \"\"\")\n",
    "    filenames = '\\n'.join(file_apis.keys())\n",
    "    apis = ''\n",
    "    for file in file_apis:\n",
    "        apis = apis + file + '\\n' + file_apis[file]\n",
    "    queries = {}\n",
    "    responses = {}\n",
    "    for file in file_apis:\n",
    "        otherfiles = [_ for _ in file_apis.keys() if _ != file]\n",
    "        user_msg = user_template.render({\"filenames\":filenames, \"apis\":apis, 'implement': file, 'otherfiles':otherfiles})\n",
    "        queries[file] = {'system': system, 'user_msg': user_msg}\n",
    "        responses[file] = callgpt(system, user_msg)\n",
    "        print(file, responses[file])\n",
    "    return queries, responses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5ccd26eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor.py {'id': 'chatcmpl-8SfnZSg22bLp4zJiBllysuoKcK63Y', 'object': 'chat.completion', 'created': 1701844549, 'model': 'gpt-4-32k', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': '```python\\nimport numpy as np\\n\\nclass Tensor:\\n    def __init__(self, data, requires_grad=False):\\n        \"\"\"\\n        Initializes the tensor with data and a boolean indicating if it requires gradient computation.\\n        \"\"\"\\n        self.data = np.array(data)\\n        self.requires_grad = requires_grad\\n        self._grad = None\\n        self._grad_fn = None\\n\\n    @property\\n    def shape(self):\\n        return self.data.shape\\n\\n    @property\\n    def grad(self):\\n        return self._grad\\n\\n    @property\\n    def grad_fn(self):\\n        if not self.requires_grad:\\n            raise Exception(\\'This tensor is not backpropagated\\')\\n        return self._grad_fn\\n\\n    def backward(self, grad=None):\\n        \"\"\"\\n        Performs backpropagation starting from this tensor.\\n        \"\"\"\\n        if not self.grad_fn:\\n            return False\\n\\n        if grad is None and self._grad is None:\\n            # if this is the end of the backpropagation we start with grad=1\\n            grad = self.__class__(1., requires_grad=False)\\n\\n        elif self.grad is not None:\\n            grad = self._grad\\n\\n        if not self.requires_grad:\\n            raise Exception(\\'This tensor is not backpropagated\\')\\n\\n        self.grad_fn.backward(grad)\\n        return True\\n\\n    def zero_grad(self):\\n        \"\"\"\\n        Zeros the gradient of the tensor.\\n        \"\"\"\\n        self._grad = None\\n\\n    def __str__(self):\\n        return f\\'Tensor({str(self.data)})\\'\\n\\n    def add_grad(self, grad):\\n        if self._grad is None:\\n            self._grad = grad\\n        else:\\n            self._grad += grad\\n\\n    def __add__(self, o): \\n        \"\"\"\\n        Overloads the addition operator\\n        \"\"\"\\n        if self.data is not None:\\n            self.data += o.data  \\n            return self\\n        self.data = o.data \\n        return self\\n\\n    def __mul__(self, o):\\n        \"\"\"\\n        Overloads the multiplication operator\\n        \"\"\"\\n        return self.__class__(self.data * o.data)\\n\\n    def __neg__(self):\\n        \"\"\"\\n        Overloads the negation operator\\n        \"\"\"\\n        return self.__class__(-self.data)\\n\\n    def __sub__(self, o):\\n        \"\"\"\\n        Overloads the subtraction operator\\n        \"\"\"\\n        return self.__add__(-o)\\n```\\nFormat example:\\n```python\\n# create a tensor with data as 1,2,3 and requires_grad set to True\\nx = Tensor(np.array([1,2,3]), True)\\nprint(x) # Tensor([1 2 3])\\n# add another tensor\\ny = x + Tensor(np.array([4,5,6]), True)\\nprint(y) # Tensor([5 7 9])\\n# multiply by another tensor\\nz = y * Tensor(np.array([7,8,9]), True)\\nprint(z) # Tensor([35 56 81])\\n# calculate gradients\\nz.backward()\\nprint(x.grad) # Tensor([ 28. 40. 54.])\\n# zero the gradients\\nx.zero_grad()\\nprint(x.grad) # None\\n```'}, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 2991, 'completion_tokens': 645, 'total_tokens': 3636}}\n",
      "autograd.py {'id': 'chatcmpl-8Sfo3sLqHivEkmg0pICdcOAp4Qkwt', 'object': 'chat.completion', 'created': 1701844579, 'model': 'gpt-4-32k', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': '```python\\n## autograd.py\\n\\nimport numpy as np\\n\\nclass Variable:\\n    \"\"\"\\n    A variable which holds a float and enables gradient computations.\\n    \"\"\"\\n\\n    def __init__(self, data: np.ndarray, requires_grad: bool = False):\\n        self.data = data\\n        self.requires_grad = requires_grad\\n        self.grad = None\\n        self.grad_fn = None\\n\\n    def backward(self, grad: np.ndarray = None):\\n        \"\"\"\\n        Computes the gradients of this variable and all variables it depends on.\\n        \"\"\"\\n\\n        if not self.requires_grad:\\n            return False\\n\\n        if grad is None and self.grad is None:\\n            # if this is the end of the backward pass\\n            grad = np.ones_like(self.data)\\n\\n        elif self.grad is not None:\\n            grad = self.grad\\n\\n        if self.grad_fn is not None:\\n            self.grad_fn.backward(grad)\\n\\n    def zero_grad(self):\\n        \"\"\"\\n        Set the gradients to zero.\\n        \"\"\"\\n\\n        self.grad = None\\n\\n\\nclass Function:\\n    \"\"\"\\n    A function which takes in variables and produces variables.\\n    \"\"\"\\n\\n    def apply(self, *args, **kwargs):\\n        result = self.forward(*args, **kwargs)\\n        if isinstance(result, Variable):\\n            result.grad_fn = self\\n        return result\\n\\n    def forward(self, *args, **kwargs):\\n        \"\"\"\\n        Computes the forward pass of the function.\\n        \"\"\"\\n        raise NotImplementedError\\n\\n    def backward(self, grad):\\n        \"\"\"\\n        Computes the backward pass of the function.\\n        \"\"\"\\n        raise NotImplementedError\\n\\n\\nclass Add(Function):\\n    \"\"\"\\n    Function to add two variables.\\n    \"\"\"\\n\\n    def forward(self, a: Variable, b: Variable):\\n        self.saved_for_backward = (a, b)\\n        return Variable(a.data + b.data, requires_grad=True)\\n\\n    def backward(self, grad: np.ndarray):\\n        a, b = self.saved_for_backward\\n        if a.requires_grad:\\n            a.grad = grad\\n            a.backward()\\n        if b.requires_grad:\\n            b.grad = grad\\n            b.backward()\\n\\n\\nclass Multiply(Function):\\n    \"\"\"\\n    Function to multiply two variables.\\n    \"\"\"\\n\\n    def forward(self, a: Variable, b: Variable):\\n        self.saved_for_backward = (a, b)\\n        return Variable(a.data * b.data, requires_grad=True)\\n\\n    def backward(self, grad: np.ndarray):\\n        a, b = self.saved_for_backward\\n        if a.requires_grad:\\n            a.grad = grad * b.data\\n            a.backward()\\n        if b.requires_grad:\\n            b.grad = grad * a.data\\n            b.backward()\\n```'}, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 2991, 'completion_tokens': 529, 'total_tokens': 3520}}\n",
      "module.py {'id': 'chatcmpl-8SfoelcLnz9T4bdgMEv2gQhfaoDQo', 'object': 'chat.completion', 'created': 1701844616, 'model': 'gpt-4-32k', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': '```python\\n## module.py\\n\\nclass Module:\\n    def __init__(self):\\n        self._parameters = {}\\n\\n    def forward(self, *input):\\n        raise NotImplementedError\\n\\n    def __call__(self, *input):\\n        return self.forward(*input)\\n\\n    def parameters(self):\\n        for name, param in self._parameters.items():\\n            yield param\\n\\n    def zero_grad(self):\\n        for param in self.parameters():\\n            param.zero_grad()\\n\\n    def add_parameter(self, name: str, param):\\n        if not isinstance(name, str):\\n            raise TypeError(\"parameter name should be a string. \"\\n                            \"Got {}\".format(type(name)))\\n        elif \\'.\\' in name:\\n            raise KeyError(\"parameter name can\\'t contain \\\\\".\\\\\"\")\\n        elif name == \\'\\':\\n            raise KeyError(\"parameter name can\\'t be empty string \\\\\"\\\\\"\")\\n        elif hasattr(self, name) and name not in self._parameters:\\n            raise KeyError(\"attribute \\'{}\\' already exists\".format(name))\\n        else:\\n            self._parameters[name] = param\\n\\nclass Linear(Module):\\n    def __init__(self, in_features: int, out_features: int):\\n        super(Linear, self).__init__()\\n        self.in_features = in_features\\n        self.out_features = out_features\\n\\n        # weights and bias are the learnable parameters\\n        self.weight = Tensor(np.random.randn(out_features, in_features), requires_grad=True)\\n        self.bias = Tensor(np.random.randn(out_features), requires_grad=True)\\n\\n        self.add_parameter(\\'weight\\', self.weight)\\n        self.add_parameter(\\'bias\\', self.bias)\\n\\n    def forward(self, input):\\n        # Applies the linear transformation to the input data\\n        return input @ self.weight.T() + self.bias\\n```'}, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 2991, 'completion_tokens': 348, 'total_tokens': 3339}}\n",
      "layers.py {'id': 'chatcmpl-8Sfp25F3TsJuVI7KStE39wYFedwrY', 'object': 'chat.completion', 'created': 1701844640, 'model': 'gpt-4-32k', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': '```python\\n## layers.py\\n\\nimport numpy as np\\nfrom module import Module\\nfrom tensor import Tensor\\nfrom autograd import Function\\n\\nclass Linear(Module):\\n    def __init__(self, in_features: int, out_features: int):\\n        super().__init__()\\n        self.in_features = in_features\\n        self.out_features = out_features\\n\\n        # Initialize weights and bias\\n        self.weight = Tensor(np.random.randn(in_features, out_features), requires_grad=True)\\n        self.bias = Tensor(np.random.randn(out_features), requires_grad=True)\\n\\n    def forward(self, input: Tensor) -> Tensor:\\n        return input @ self.weight + self.bias\\n\\nclass ReLU(Module):\\n    def __init__(self):\\n        super().__init__()\\n\\n    def forward(self, input: Tensor) -> Tensor:\\n        return ReLUFunction.apply(input)\\n\\nclass ReLUFunction(Function):\\n    @staticmethod\\n    def forward(ctx, input: Tensor) -> Tensor:\\n        ctx.save_for_backward(input)\\n        return input.clamp_min(0)\\n\\n    @staticmethod\\n    def backward(ctx, grad_output: Tensor) -> Tensor:\\n        input, = ctx.saved_tensors\\n        grad_input = grad_output.clone()\\n        grad_input[input < 0] = 0\\n        return grad_input\\n\\nclass Sigmoid(Module):\\n    def __init__(self):\\n        super().__init__()\\n\\n    def forward(self, input: Tensor) -> Tensor:\\n        return SigmoidFunction.apply(input)\\n\\nclass SigmoidFunction(Function):\\n    @staticmethod\\n    def forward(ctx, input: Tensor) -> Tensor:\\n        output = 1 / (1 + np.exp(-input.data))\\n        ctx.save_for_backward(output)\\n        return Tensor(output, requires_grad=input.requires_grad)\\n\\n    @staticmethod\\n    def backward(ctx, grad_output: Tensor) -> Tensor:\\n        output, = ctx.saved_tensors\\n        return grad_output * output * (1 - output)\\n\\nclass Tanh(Module):\\n    def __init__(self):\\n        super().__init__()\\n\\n    def forward(self, input: Tensor) -> Tensor:\\n        return TanhFunction.apply(input)\\n\\nclass TanhFunction(Function):\\n    @staticmethod\\n    def forward(ctx, input: Tensor) -> Tensor:\\n        output = np.tanh(input.data)\\n        ctx.save_for_backward(output)\\n        return Tensor(output, requires_grad=input.requires_grad)\\n\\n    @staticmethod\\n    def backward(ctx, grad_output: Tensor) -> Tensor:\\n        output, = ctx.saved_tensors\\n        return grad_output * (1 - output ** 2)\\n```'}, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 2991, 'completion_tokens': 511, 'total_tokens': 3502}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optim.py {'id': 'chatcmpl-8Sfpfzbnntd6uBMt2IprvzvyeoKfy', 'object': 'chat.completion', 'created': 1701844679, 'model': 'gpt-4-32k', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': '```python\\n## Code for optim.py\\nimport numpy as np\\n\\nclass Tensor:\\n    def __init__(self, data: np.ndarray, requires_grad: bool = False):\\n        self.data = data\\n        self.requires_grad = requires_grad\\n        self.grad = None if not requires_grad else np.zeros_like(data)\\n\\n    def zero_grad(self):\\n        if self.grad is not None:\\n            self.grad.fill(0)\\n\\nclass Optimizer:\\n    def __init__(self, params, lr: float = 0.001):\\n        self.params = params\\n        self.lr = lr\\n\\n    def step(self):\\n        for param in self.params:\\n            if param.requires_grad:\\n                param.data -= self.lr * param.grad\\n\\n    def zero_grad(self):\\n        for param in self.params:\\n            param.zero_grad()\\n```\\nFormat example:\\n\\n```python\\nparams = [Tensor(np.array([1.0, 2.0, 3.0]), requires_grad=True), Tensor(np.array([4.0, 5.0, 6.0]), requires_grad=True)]\\noptimizer = Optimizer(params, lr=0.01)\\nfor param in params:\\n    param.grad = np.array([0.1, 0.1, 0.1])\\noptimizer.step()\\nfor param in params:\\n    print(param.data)  # prints: [0.99 1.99 2.99] and [3.99 4.99 5.99]\\noptimizer.zero_grad()\\nfor param in params:\\n    print(param.grad)  # prints: [0. 0. 0.] and [0. 0. 0.]\\n```'}, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 2991, 'completion_tokens': 342, 'total_tokens': 3333}}\n",
      "utils.py {'id': 'chatcmpl-8Sfpr1r9HzSG1kGTd3k0oYZYvyYrW', 'object': 'chat.completion', 'created': 1701844691, 'model': 'gpt-4-32k', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': '```python\\n## utils.py\\n\\nimport pickle\\nfrom typing import Any\\nfrom module import Module\\n\\ndef save_model(model: Module, file_path: str) -> None:\\n    \"\"\"\\n    Saves the model to a file\\n    :param model: The model to be saved\\n    :param file_path: The path of the file\\n    :return: None\\n    \"\"\"\\n    with open(file_path, \\'wb\\') as f:\\n        pickle.dump(model, f)\\n\\ndef load_model(file_path: str) -> Module:\\n    \"\"\"\\n    Loads a model from a file\\n    :param file_path: The path of the file\\n    :return: The loaded model\\n    \"\"\"\\n    with open(file_path, \\'rb\\') as f:\\n        model = pickle.load(f)\\n    return model\\n\\ndef split_dataset(dataset: Any, ratio: float=0.8) -> tuple:\\n    \"\"\"\\n    Splits a dataset into training and testing sets\\n    :param dataset: The dataset to be split\\n    :param ratio: The ratio of the training set to the whole dataset\\n    :return: A tuple (training_set, testing_set)\\n    \"\"\"\\n    train_size = int(len(dataset) * ratio)\\n    train_set = dataset[:train_size]\\n    test_set = dataset[train_size:]\\n    return train_set, test_set\\n\\ndef normalize_data(data: Any) -> Any:\\n    \"\"\"\\n    Normalizes data to have zero mean and unit variance\\n    :param data: The data to be normalized\\n    :return: The normalized data\\n    \"\"\"\\n    mean = data.mean()\\n    std = data.std()\\n    return (data - mean) / std\\n```'}, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 2991, 'completion_tokens': 338, 'total_tokens': 3329}}\n",
      "dataloader.py {'id': 'chatcmpl-8SfqFpNmiChPtxug4B6nUqpoMRrsz', 'object': 'chat.completion', 'created': 1701844715, 'model': 'gpt-4-32k', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': '```python\\n## Import necessary libraries\\nimport numpy as np\\nfrom typing import List, Tuple\\nfrom itertools import islice\\n\\nclass DataLoader:\\n    def __init__(self, dataset: List[Tuple[np.array, int]], batch_size: int=1, shuffle: bool=False):\\n        \"\"\"\\n        Initializes the dataloader with a dataset, batch size, and shuffle option\\n\\n        Args:\\n            dataset: List of tuples, where each tuple contains a numpy array (data point) and an integer (label)\\n            batch_size: Number of samples per batch\\n            shuffle: Whether to shuffle the dataset before creating batches\\n        \"\"\"\\n        self.dataset = dataset\\n        self.batch_size = batch_size\\n        self.shuffle = shuffle\\n\\n    def __iter__(self):\\n        \"\"\"\\n        Provides an iterator to load data in batches\\n\\n        Yields:\\n            Batches of data points and labels\\n        \"\"\"\\n        # If shuffle is enabled, shuffle the dataset before creating batches\\n        if self.shuffle:\\n            np.random.shuffle(self.dataset)\\n\\n        # Yield batches\\n        for i in range(0, len(self.dataset), self.batch_size):\\n            data_batch, labels_batch = zip(*islice(self.dataset, i, i + self.batch_size))\\n            yield np.array(data_batch), np.array(labels_batch)\\n```'}, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 2992, 'completion_tokens': 264, 'total_tokens': 3256}}\n",
      "main.py {'id': 'chatcmpl-8SfqRtol9GcXrikPMsdtFVEtkpi5h', 'object': 'chat.completion', 'created': 1701844727, 'model': 'gpt-4-32k', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': '```python\\nimport torch\\nfrom torch import nn, optim\\nfrom torch.utils.data import DataLoader\\nfrom torchvision import datasets, transforms\\n\\nclass MLP(nn.Module):\\n    def __init__(self):\\n        super(MLP, self).__init__()\\n        self.model = nn.Sequential(\\n            nn.Linear(784, 200),\\n            nn.LeakyReLU(inplace=True),\\n            nn.Linear(200, 200),\\n            nn.LeakyReLU(inplace=True),\\n            nn.Linear(200, 10),\\n            nn.LeakyReLU(inplace=True),\\n        )\\n\\n    def forward(self, x):\\n        return self.model(x)\\n\\ndef main():\\n    batch_size = 200\\n    learning_rate = 0.01\\n    epochs = 10\\n\\n    train_loader = DataLoader(\\n        datasets.MNIST(\\'./data\\', train=True, download=True,\\n                       transform=transforms.Compose([\\n                           transforms.ToTensor(),\\n                           transforms.Normalize((0.1307,), (0.3081,))\\n                       ])),\\n        batch_size=batch_size, shuffle=True)\\n\\n    test_loader = DataLoader(\\n        datasets.MNIST(\\'./data\\', train=False, transform=transforms.Compose([\\n            transforms.ToTensor(),\\n            transforms.Normalize((0.1307,), (0.3081,))\\n        ])),\\n        batch_size=batch_size, shuffle=True)\\n\\n    device = torch.device(\\'cuda:0\\')\\n    net = MLP().to(device)\\n    optimizer = optim.SGD(net.parameters(), lr=learning_rate)\\n    criteon = nn.CrossEntropyLoss().to(device)\\n\\n    for epoch in range(epochs):\\n\\n        for batch_idx, (data, target) in enumerate(train_loader):\\n            data = data.view(-1, 28*28)\\n            data, target = data.to(device), target.to(device)\\n\\n            logits = net(data)\\n            loss = criteon(logits, target)\\n\\n            optimizer.zero_grad()\\n            loss.backward()\\n            optimizer.step()\\n\\n            if batch_idx % 100 == 0:\\n                print(\\'Train Epoch: {} [{}/{} ({:.0f}%)]\\\\tLoss: {:.6f}\\'.format(\\n                    epoch, batch_idx * len(data), len(train_loader.dataset),\\n                           100. * batch_idx / len(train_loader), loss.item()))\\n\\n        test_loss = 0\\n        correct = 0\\n        for data, target in test_loader:\\n            data = data.view(-1, 28 * 28)\\n            data, target = data.to(device), target.to(device)\\n            logits = net(data)\\n            test_loss += criteon(logits, target).item()\\n\\n            pred = logits.data.max(1)[1]\\n            correct += pred.eq(target.data).sum()\\n\\n        test_loss /= len(test_loader.dataset)\\n        print(\\'\\\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\\\n\\'.format(\\n            test_loss, correct, len(test_loader.dataset),\\n            100. * correct / len(test_loader.dataset)))\\n\\nif __name__ == \"__main__\":\\n    main()\\n```'}, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 2991, 'completion_tokens': 610, 'total_tokens': 3601}}\n"
     ]
    }
   ],
   "source": [
    "r1_queries, r1_responses = round1(file_apis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "72405668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "import numpy as np\n",
      "\n",
      "class Tensor:\n",
      "    def __init__(self, data, requires_grad=False):\n",
      "        \"\"\"\n",
      "        Initializes the tensor with data and a boolean indicating if it requires gradient computation.\n",
      "        \"\"\"\n",
      "        self.data = np.array(data)\n",
      "        self.requires_grad = requires_grad\n",
      "        self._grad = None\n",
      "        self._grad_fn = None\n",
      "\n",
      "    @property\n",
      "    def shape(self):\n",
      "        return self.data.shape\n",
      "\n",
      "    @property\n",
      "    def grad(self):\n",
      "        return self._grad\n",
      "\n",
      "    @property\n",
      "    def grad_fn(self):\n",
      "        if not self.requires_grad:\n",
      "            raise Exception('This tensor is not backpropagated')\n",
      "        return self._grad_fn\n",
      "\n",
      "    def backward(self, grad=None):\n",
      "        \"\"\"\n",
      "        Performs backpropagation starting from this tensor.\n",
      "        \"\"\"\n",
      "        if not self.grad_fn:\n",
      "            return False\n",
      "\n",
      "        if grad is None and self._grad is None:\n",
      "            # if this is the end of the backpropagation we start with grad=1\n",
      "            grad = self.__class__(1., requires_grad=False)\n",
      "\n",
      "        elif self.grad is not None:\n",
      "            grad = self._grad\n",
      "\n",
      "        if not self.requires_grad:\n",
      "            raise Exception('This tensor is not backpropagated')\n",
      "\n",
      "        self.grad_fn.backward(grad)\n",
      "        return True\n",
      "\n",
      "    def zero_grad(self):\n",
      "        \"\"\"\n",
      "        Zeros the gradient of the tensor.\n",
      "        \"\"\"\n",
      "        self._grad = None\n",
      "\n",
      "    def __str__(self):\n",
      "        return f'Tensor({str(self.data)})'\n",
      "\n",
      "    def add_grad(self, grad):\n",
      "        if self._grad is None:\n",
      "            self._grad = grad\n",
      "        else:\n",
      "            self._grad += grad\n",
      "\n",
      "    def __add__(self, o): \n",
      "        \"\"\"\n",
      "        Overloads the addition operator\n",
      "        \"\"\"\n",
      "        if self.data is not None:\n",
      "            self.data += o.data  \n",
      "            return self\n",
      "        self.data = o.data \n",
      "        return self\n",
      "\n",
      "    def __mul__(self, o):\n",
      "        \"\"\"\n",
      "        Overloads the multiplication operator\n",
      "        \"\"\"\n",
      "        return self.__class__(self.data * o.data)\n",
      "\n",
      "    def __neg__(self):\n",
      "        \"\"\"\n",
      "        Overloads the negation operator\n",
      "        \"\"\"\n",
      "        return self.__class__(-self.data)\n",
      "\n",
      "    def __sub__(self, o):\n",
      "        \"\"\"\n",
      "        Overloads the subtraction operator\n",
      "        \"\"\"\n",
      "        return self.__add__(-o)\n",
      "```\n",
      "Format example:\n",
      "```python\n",
      "# create a tensor with data as 1,2,3 and requires_grad set to True\n",
      "x = Tensor(np.array([1,2,3]), True)\n",
      "print(x) # Tensor([1 2 3])\n",
      "# add another tensor\n",
      "y = x + Tensor(np.array([4,5,6]), True)\n",
      "print(y) # Tensor([5 7 9])\n",
      "# multiply by another tensor\n",
      "z = y * Tensor(np.array([7,8,9]), True)\n",
      "print(z) # Tensor([35 56 81])\n",
      "# calculate gradients\n",
      "z.backward()\n",
      "print(x.grad) # Tensor([ 28. 40. 54.])\n",
      "# zero the gradients\n",
      "x.zero_grad()\n",
      "print(x.grad) # None\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(r1_responses['tensor.py']['choices'][0]['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "086bdc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def round2(file_codes):\n",
    "    system = \"\"\"\n",
    "You are a professional software engineer, and your main task is to review the code. You need to ensure that the code conforms to the PEP8 standards, is elegantly designed and modularized, easy to read and maintain, and is written in Python 3.9 (or in another programming language).\n",
    "ATTENTION: Use '##' to SPLIT SECTIONS, not '#'. Output format carefully referenced \"Format example\".\n",
    "\n",
    "## Code Review: Based on the following context and code, and following the check list, Provide key, clear, concise, and specific code modification suggestions, up to 5.\n",
    "```\n",
    "1. Check 0: Is the code implemented as per the requirements?\n",
    "2. Check 1: Are there any issues with the code logic?\n",
    "3. Check 2: Does the existing code follow the \"Data structures and interface definitions\"?\n",
    "4. Check 3: Is there a function in the code that is omitted or not fully implemented that needs to be implemented?\n",
    "5. Check 4: Does the code have unnecessary or lack dependencies?\n",
    "\n",
    "## Format example\n",
    "\n",
    "## Code Review\n",
    "1. The code ...\n",
    "2. ...\n",
    "3. ...\n",
    "4. ...\n",
    "5. ...\n",
    "\"\"\"\n",
    "    user_template = Template(\"\"\"\n",
    "    Task: Write a deep learning framework like PyTorch \n",
    "\n",
    "    Codebase:\n",
    "    {{codebase}}\n",
    "\n",
    "    Based on the codebase, you only need to give advice on {{file}}.\n",
    "    Do not give advice any content in {{otherfiles}}. Strictly follow the response format.\n",
    "    Do not answer any other content or suggestions.\n",
    "    \"\"\")\n",
    "    codebase = ''\n",
    "    queries = {}\n",
    "    responses = {}\n",
    "    \n",
    "    for file in file_codes:\n",
    "        codebase = codebase +'\\n' + file + '\\n' + file_codes[file]\n",
    "    for file in file_apis:\n",
    "        otherfiles = [_ for _ in file_apis.keys() if _ != file]\n",
    "        user_msg = user_template.render({\"codebase\": codebase, 'file': file, 'otherfiles':otherfiles})\n",
    "        queries[file] = {'system': system, 'user_msg': user_msg}        \n",
    "        responses[file] = callgpt(system, user_msg)\n",
    "        print(file, responses[file])\n",
    "    return queries, responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "896e86f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor.py {'id': 'chatcmpl-8SfwFYaejtivKVMSMgBQqryBO79cd', 'object': 'chat.completion', 'created': 1701845087, 'model': 'gpt-4-32k', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': \"## Code Review\\n1. The `__add__` method in the `Tensor` class modifies the data of the object it is called on. This is counter-intuitive, as the '+' operator in Python usually doesn't modify the operands. Instead, it should return a new `Tensor` object. The same applies to the `__mul__`, `__neg__`, and `__sub__` methods.\\n\\n2. The `add_grad` method currently adds the incoming gradient to the existing gradient. However, it doesn't check if the incoming gradient and the existing gradient have the same shape. This could lead to issues down the line.\\n\\n3. The `backward` method in the `Tensor` class doesn't update the gradients of the tensor it is called upon. It only calls the `backward` method of its `_grad_fn`. It should also update its own gradients based on the incoming gradient.\\n\\n4. The `backward` method does not handle cases where the tensor does not require gradients but its `_grad_fn` is not None.\\n\\n5. The `Tensor` class doesn't have a mechanism to handle operations with non-tensor objects. For example, adding a scalar to a tensor is a very common operation but the current implementation of the `__add__` method does not support this.\"}, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 3965, 'completion_tokens': 263, 'total_tokens': 4228}}\n",
      "autograd.py {'id': 'chatcmpl-8SfwT2489RhzUzOo31vzvXKpBLVU2', 'object': 'chat.completion', 'created': 1701845101, 'model': 'gpt-4-32k', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': '## Code Review\\n\\n1. In the `Variable` class in `autograd.py`, the `grad` attribute is not initially set to None for variables that do not require gradients. This can lead to confusion and potential errors down the line. It would be better to initially set `self.grad = None` for all Variables, regardless of whether they require gradients or not.\\n\\n2. The `backward` function in the `Variable` class does not handle the case where the gradient is provided and the Variable already has a gradient. In this case, the existing gradient is replaced by the provided one, which may not be the expected behaviour. Instead, the gradients should be added together.\\n\\n3. In the `Add` and `Multiply` classes in `autograd.py`, the backward functions do not check if the gradient is None before performing the backward operation. If the gradient is None, this can lead to a TypeError. A check should be added to ensure that the gradient is not None before the backward operation is performed.\\n\\n4. The `Function` class in `autograd.py` does not provide a clear interface for saving and retrieving tensors for the backward pass. It would be more intuitive to include `save_for_backward` and `saved_tensors` methods in the `Function` class to handle this, similar to how PyTorch handles it.\\n\\n5. The `backward` methods in the `Add` and `Multiply` classes in `autograd.py` do not handle the case where the inputs do not require gradients. Currently, the backward operations are performed regardless of whether the inputs require gradients or not. This can lead to unnecessary computations. It would be better to add a check to ensure that the backward operation is only performed on inputs that require gradients.'}, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 3965, 'completion_tokens': 354, 'total_tokens': 4319}}\n",
      "module.py {'id': 'chatcmpl-8Sfx148BKYIIgK5vTli6nJWr38zeX', 'object': 'chat.completion', 'created': 1701845135, 'model': 'gpt-4-32k', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': \"## Code Review\\n1. The code in `module.py` follows the requirements well. The `Module` class is correctly abstracted to serve as a base class for different layers in a deep learning model. The `Linear` class correctly inherits from `Module` and implements the `forward` method for linear transformation.\\n   \\n2. The logic of the code seems robust, but there's one potential issue in the `forward` method of the `Linear` class. The method assumes that the `input` argument is a `Tensor` object that has a `@` method for matrix multiplication and a `T()` method for transposition. If the `input` object does not have these methods, the code will crash. This could be made more robust by explicitly checking the type of `input` or by using standard numpy operations instead of relying on methods of `input`.\\n   \\n3. The existing code follows the data structures and interface definitions well. There is an abstract `Module` base class and a `Linear` subclass that correctly implements the `forward` method. The `add_parameter` method is correctly implemented to save the parameters that need to be learned.\\n   \\n4. There is no need for additional functions in the provided code. All the necessary methods for the `Module` and `Linear` classes have been implemented.\\n   \\n5. The code does not have unnecessary dependencies. It does not import any Python packages that it does not use. However, it assumes that the `Tensor` class has certain methods (`@`, `T()`, and `+`), which could be considered as implicit dependencies.\"}, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 3965, 'completion_tokens': 324, 'total_tokens': 4289}}\n",
      "layers.py {'id': 'chatcmpl-8SfxFiMzn5yLBIYhiwM7LJd87W7Sg', 'object': 'chat.completion', 'created': 1701845149, 'model': 'gpt-4-32k', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': \"## Code Review\\n1. The forward method of Linear class in layers.py is making use of the @ operator for matrix multiplication. This operator is not supported in Python 3.4 and below. Make sure to use numpy's dot function for compatibility with Python versions below 3.5. Replace `input @ self.weight` with `input.dot(self.weight)`.\\n\\n2. The ReLUFunction's backward method in layers.py sets gradients to 0 where the input is less than 0. This might not be appropriate for negative slope ReLU (Leaky ReLU). If negative slope ReLU is required, this logic needs to be updated to handle negative input values differently.\\n\\n3. The SigmoidFunction in layers.py does not check if the input tensor requires gradient computation before performing the operations. You should add a condition to check if `requires_grad` is True before performing operations.\\n\\n4. The TanhFunction in layers.py does not check if the input tensor requires gradient computation before performing the operations. You should add a condition to check if `requires_grad` is True before performing operations.\\n\\n5. The backward method of TanhFunction in layers.py calculates the gradients even when they are not required. This can be optimized by checking if the gradients are required before performing the computation. Add a condition to check if `requires_grad` is True before performing operations.\"}, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 3965, 'completion_tokens': 275, 'total_tokens': 4240}}\n",
      "optim.py {'id': 'chatcmpl-8SfxXylPCgndqSjg9QI9LvR2DZIqd', 'object': 'chat.completion', 'created': 1701845167, 'model': 'gpt-4-32k', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': \"## Code Review\\n1. The code has not been implemented according to the PEP8 standards. The comments should start with a capital letter and end with a period. This makes the code more readable and maintains the uniformity of the codebase.  \\n2. The 'Tensor' class is repeated in 'optim.py'. It is unnecessary and might lead to confusion. The 'Tensor' class should be imported from 'tensor.py' instead of redefining it in 'optim.py'.\\n3. The type hints for the 'params' in the Optimizer class constructor are missing. It is better to add type hints to make the code easier to understand and debug.\\n4. In the 'step' function of the 'Optimizer' class, there is no check whether the 'grad' is 'None'. Before subtracting the gradient from the data, a check should be added to ensure that the gradient is not 'None'. If it is 'None', an exception should be raised.\\n5. There are no docstrings in the functions of the 'Optimizer' class. Adding docstrings would make it easier to understand the purpose of each function.\"}, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 3965, 'completion_tokens': 230, 'total_tokens': 4195}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "utils.py {'id': 'chatcmpl-8Sfxi17yjafEjcVsPmUWdfbt2MRpu', 'object': 'chat.completion', 'created': 1701845178, 'model': 'gpt-4-32k', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': '## Code Review\\n1. The code in utils.py does not seem to have any notable issues with the code logic. However, without knowing the full context of the project, it might be useful to add type checking to ensure that the \"dataset\" parameter in the \"split_dataset\" function is iterable. This could prevent potential runtime errors.\\n2. The function definitions in utils.py do not follow the PEP8 standards for docstrings. According to PEP8, the description in the docstring should start in the line immediately after the triple quotes. Also, each parameter should be described in the docstring.\\n3. There are no function definitions in utils.py that are omitted or not fully implemented. However, the \"normalize_data\" function does not handle the case where the standard deviation of the data is zero. This could cause a ZeroDivisionError at runtime.\\n4. The \"normalize_data\" function in utils.py lacks error handling for edge cases, such as when the input data is an empty list or when the standard deviation of the data is zero. It could be improved by adding condition checks to handle these scenarios.\\n5. The utils.py file does not appear to have any unnecessary dependencies. It uses only the \"pickle\" and \"typing\" modules from the Python Standard Library. However, it is not clear from the given code whether the use of these libraries is appropriate, as that would depend on the wider context of the project.'}, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 3965, 'completion_tokens': 290, 'total_tokens': 4255}}\n",
      "dataloader.py {'id': 'chatcmpl-8SfxwNj1Xsz6drC98wkMhglRJKUAJ', 'object': 'chat.completion', 'created': 1701845192, 'model': 'gpt-4-32k', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': \"## Code Review\\n\\n1. The `DataLoader` class currently doesn't implement the `__len__` method which would be useful to get the number of batches in the loader. This method can be implemented as follows:\\n\\n```python\\ndef __len__(self):\\n    return (len(self.dataset) + self.batch_size - 1) // self.batch_size  # rounds up\\n```\\n\\n2. In the `__iter__` method, `np.random.shuffle(self.dataset)` is used to shuffle the dataset. However, this would modify the original dataset. To avoid this, you can use `random.sample` from the Python's built-in `random` module, which returns a new shuffled list and leaves the original list unmodified:\\n\\n```python\\nif self.shuffle:\\n    self.dataset = random.sample(self.dataset, len(self.dataset))\\n```\\n\\n3. The `DataLoader` class currently assumes that the dataset is a list of tuples. However, it would be more flexible if the class can also accept other iterable types such as generators. This could be done by checking the type of the dataset before trying to get its length or shuffle it.\\n\\n4. The code in the `__iter__` method can be simplified using `itertools.islice` to generate batches, avoiding the need for manual indexing:\\n\\n```python\\nfor i in range(0, len(self.dataset), self.batch_size):\\n    yield list(islice(self.dataset, i, i + self.batch_size))\\n```\\n\\n5. The `DataLoader` currently returns batches as lists of numpy arrays. It might be beneficial to convert these to Tensor objects before returning, if the deep learning framework being developed uses Tensor objects for computations. \\n\\n```python\\nyield Tensor(np.array(data_batch)), Tensor(np.array(labels_batch))\\n```\\n\\nIf Tensor objects are used, remember to import the Tensor class at the beginning of the code.\"}, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 3966, 'completion_tokens': 381, 'total_tokens': 4347}}\n",
      "main.py {'id': 'chatcmpl-8Sfy92ATWcE8HtYSdTiso4Np8kwVH', 'object': 'chat.completion', 'created': 1701845205, 'model': 'gpt-4-32k', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': \"## Code Review\\n1. The code in main.py does not appear to follow PEP8 standards. According to PEP8, there should be two blank lines between the import statements and the first line of actual code, in this case, the class definition for MLP. This can be fixed by adding an extra line between the import statements and the MLP class definition.\\n2. The code does not have any comments that explain what is happening in each block of code. This can make the code difficult to read and maintain. Consider adding comments to clarify the purpose of each section of code.\\n3. The main function is quite long and does multiple things. According to best practices for modular design, each function should only do one thing. Consider breaking the main function into smaller functions that each perform a specific task. For example, you could create a separate function for loading the data, another function for training the model, and another function for testing the model.\\n4. The code in main.py does not have any error handling. If something goes wrong, such as a file not being found or a model not training correctly, the code will simply crash. Consider adding try/except blocks to handle potential errors and exceptions gracefully.\\n5. The code currently uses the 'cuda:0' device for training the model. This assumes that there is a CUDA-capable GPU available on the machine running the code. This may not always be the case. Consider adding a check to see if a CUDA-capable GPU is available, and if not, default to using the CPU for training.\"}, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 3965, 'completion_tokens': 315, 'total_tokens': 4280}}\n"
     ]
    }
   ],
   "source": [
    "file_codes_r1 = {}\n",
    "for file in file_apis:\n",
    "    file_codes_r1[file] = r1_responses[file]['choices'][0]['message']['content']\n",
    "r2_queries, r2_responses = round2(file_codes_r1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0717894c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def round3(file_codes, file_reviews):\n",
    "    system = \"\"\"\n",
    "NOTICE\n",
    "Role: You are a professional engineer; the main goal is to write PEP8 compliant, elegant, modular, easy to read and maintain Python 3.9 code (but you can also use other programming language)\n",
    "ATTENTION: Use '##' to SPLIT SECTIONS, not '#'. Output format carefully referenced \"Format example\".\n",
    "\n",
    "## Code: Write code with triple quoto, based on the following list and context.\n",
    "1. Do your best to implement THIS ONLY ONE FILE. ONLY USE EXISTING API. IF NO API, IMPLEMENT IT.\n",
    "2. Requirement: Based on the context, implement one following code file, note to return only in code form, your code will be part of the entire project, so please implement complete, reliable, reusable code snippets\n",
    "3. Attention1: If there is any setting, ALWAYS SET A DEFAULT VALUE, ALWAYS USE STRONG TYPE AND EXPLICIT VARIABLE.\n",
    "4. Attention2: YOU MUST FOLLOW \"Data structures and interface definitions\". DONT CHANGE ANY DESIGN.\n",
    "5. Think before writing: What should be implemented and provided in this document?\n",
    "6. CAREFULLY CHECK THAT YOU DONT MISS ANY NECESSARY CLASS/FUNCTION IN THIS FILE.\n",
    "7. Do not use public member functions that do not exist in your design.\n",
    "\"\"\"\n",
    "    user_template = Template(\"\"\"\n",
    "    Task: Write a deep learning framework like PyTorch \n",
    "\n",
    "    Codebase:\n",
    "    {{codebase}}\n",
    "    \n",
    "    Reviews:\n",
    "    {{review}}\n",
    "\n",
    "    Based on the codebase, and review comments, revise your past implementation of {{file}}. \n",
    "    You only need to implement {{file}}. Implement all functions and additional functions you need. DO NOT LET ME TO IMPLEMENT ANYTHING!!!!\n",
    "    Make sure your response code is runnable.\n",
    "    Do not response any content in {{otherfiles}}. Strictly follow the response format. Do not answer any other content or suggestions.\n",
    "    \"\"\")\n",
    "    codebase = ''\n",
    "    reviews = ''\n",
    "    queries = {}\n",
    "    responses = {}\n",
    "    \n",
    "    for file in file_codes:\n",
    "        codebase = codebase +'\\n' + file + '\\n' + file_codes[file]\n",
    "        reviews = reviews + '\\n' + file + '\\n' + file_reviews[file]\n",
    "        \n",
    "    for file in file_apis:\n",
    "        otherfiles = [_ for _ in file_apis.keys() if _ != file]\n",
    "        user_msg = user_template.render({\"codebase\": codebase, 'review': reviews, 'file': file, 'otherfiles':otherfiles})\n",
    "        queries[file] = {'system': system, 'user_msg': user_msg}        \n",
    "        responses[file] = callgpt(system, user_msg)\n",
    "        print(file, responses[file])\n",
    "    return queries, responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7e040f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor.py {'id': 'chatcmpl-8Sg3yVxkhBoOsWH8uVOVuZa2h0lxA', 'object': 'chat.completion', 'created': 1701845566, 'model': 'gpt-4-32k', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': '```python\\nimport numpy as np\\n\\nclass Tensor:\\n    def __init__(self, data, requires_grad=False):\\n        \"\"\"\\n        Initializes the tensor with data and a boolean indicating if it requires gradient computation.\\n        \"\"\"\\n        self.data = np.array(data)\\n        self.requires_grad = requires_grad\\n        self._grad = None\\n        self._grad_fn = None\\n\\n    @property\\n    def shape(self):\\n        return self.data.shape\\n\\n    @property\\n    def grad(self):\\n        return self._grad\\n\\n    @property\\n    def grad_fn(self):\\n        if not self.requires_grad:\\n            raise Exception(\\'This tensor is not backpropagated\\')\\n        return self._grad_fn\\n\\n    def backward(self, grad=None):\\n        \"\"\"\\n        Performs backpropagation starting from this tensor.\\n        \"\"\"\\n        if not self.grad_fn:\\n            return False\\n\\n        if grad is None and self._grad is None:\\n            # if this is the end of the backpropagation we start with grad=1\\n            grad = self.__class__(1., requires_grad=False)\\n\\n        elif self.grad is not None:\\n            grad = self._grad\\n\\n        if not self.requires_grad:\\n            raise Exception(\\'This tensor is not backpropagated\\')\\n\\n        self.grad_fn.backward(grad)\\n        return True\\n\\n    def zero_grad(self):\\n        \"\"\"\\n        Zeros the gradient of the tensor.\\n        \"\"\"\\n        self._grad = None\\n\\n    def __str__(self):\\n        return f\\'Tensor({str(self.data)})\\'\\n\\n    def add_grad(self, grad):\\n        if self._grad is None:\\n            self._grad = grad\\n        else:\\n            if self._grad.shape != grad.shape:\\n                raise ValueError(\"Shapes of the existing gradient and the incoming gradient do not match.\")\\n            self._grad += grad\\n\\n    def __add__(self, o): \\n        \"\"\"\\n        Overloads the addition operator\\n        \"\"\"\\n        return self.__class__(self.data + o.data, requires_grad=self.requires_grad)\\n\\n    def __mul__(self, o):\\n        \"\"\"\\n        Overloads the multiplication operator\\n        \"\"\"\\n        return self.__class__(self.data * o.data, requires_grad=self.requires_grad)\\n\\n    def __neg__(self):\\n        \"\"\"\\n        Overloads the negation operator\\n        \"\"\"\\n        return self.__class__(-self.data, requires_grad=self.requires_grad)\\n\\n    def __sub__(self, o):\\n        \"\"\"\\n        Overloads the subtraction operator\\n        \"\"\"\\n        return self.__class__(self.data - o.data, requires_grad=self.requires_grad)\\n```\\nFormat example:\\n```python\\n# create a tensor with data as 1,2,3 and requires_grad set to True\\nx = Tensor(np.array([1,2,3]), True)\\nprint(x) # Tensor([1 2 3])\\n# add another tensor\\ny = x + Tensor(np.array([4,5,6]), True)\\nprint(y) # Tensor([5 7 9])\\n# multiply by another tensor\\nz = y * Tensor(np.array([7,8,9]), True)\\nprint(z) # Tensor([35 56 81])\\n# calculate gradients\\nz.backward()\\nprint(x.grad) # Tensor([ 28. 40. 54.])\\n# zero the gradients\\nx.zero_grad()\\nprint(x.grad) # None\\n```'}, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 6487, 'completion_tokens': 681, 'total_tokens': 7168}}\n",
      "autograd.py {'id': 'chatcmpl-8Sg4VAxVmCzNkBEHypkUSRO8cEySf', 'object': 'chat.completion', 'created': 1701845599, 'model': 'gpt-4-32k', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': '```python\\n## Revised implementation of autograd.py\\n\\nimport numpy as np\\n\\nclass Variable:\\n    \"\"\"\\n    A variable which holds a float and enables gradient computations.\\n    \"\"\"\\n\\n    def __init__(self, data: np.ndarray, requires_grad: bool = False):\\n        self.data = data\\n        self.requires_grad = requires_grad\\n        self.grad = None if not requires_grad else np.zeros_like(data)\\n        self.grad_fn = None\\n\\n    def backward(self, grad: np.ndarray = None):\\n        \"\"\"\\n        Computes the gradients of this variable and all variables it depends on.\\n        \"\"\"\\n\\n        if not self.requires_grad:\\n            return False\\n\\n        if grad is None and self.grad is None:\\n            # if this is the end of the backward pass\\n            grad = np.ones_like(self.data)\\n\\n        elif self.grad is not None:\\n            grad = self.grad\\n\\n        if self.grad_fn is not None:\\n            self.grad_fn.backward(grad)\\n\\n    def zero_grad(self):\\n        \"\"\"\\n        Set the gradients to zero.\\n        \"\"\"\\n\\n        self.grad = None if not self.requires_grad else np.zeros_like(self.data)\\n\\n\\nclass Function:\\n    \"\"\"\\n    A function which takes in variables and produces variables.\\n    \"\"\"\\n\\n    def apply(self, *args, **kwargs):\\n        result = self.forward(*args, **kwargs)\\n        if isinstance(result, Variable):\\n            result.grad_fn = self\\n        return result\\n\\n    def forward(self, *args, **kwargs):\\n        \"\"\"\\n        Computes the forward pass of the function.\\n        \"\"\"\\n        raise NotImplementedError\\n\\n    def backward(self, grad):\\n        \"\"\"\\n        Computes the backward pass of the function.\\n        \"\"\"\\n        raise NotImplementedError\\n\\n\\nclass Add(Function):\\n    \"\"\"\\n    Function to add two variables.\\n    \"\"\"\\n\\n    def forward(self, a: Variable, b: Variable):\\n        self.saved_for_backward = (a, b)\\n        return Variable(a.data + b.data, requires_grad=a.requires_grad or b.requires_grad)\\n\\n    def backward(self, grad: np.ndarray):\\n        a, b = self.saved_for_backward\\n        if a.requires_grad:\\n            a.grad = grad\\n            a.backward(grad)\\n        if b.requires_grad:\\n            b.grad = grad\\n            b.backward(grad)\\n\\n\\nclass Multiply(Function):\\n    \"\"\"\\n    Function to multiply two variables.\\n    \"\"\"\\n\\n    def forward(self, a: Variable, b: Variable):\\n        self.saved_for_backward = (a, b)\\n        return Variable(a.data * b.data, requires_grad=a.requires_grad or b.requires_grad)\\n\\n    def backward(self, grad: np.ndarray):\\n        a, b = self.saved_for_backward\\n        if a.requires_grad:\\n            a.grad = grad * b.data\\n            a.backward(grad * b.data)\\n        if b.requires_grad:\\n            b.grad = grad * a.data\\n            b.backward(grad * a.data)\\n```'}, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 6488, 'completion_tokens': 574, 'total_tokens': 7062}}\n",
      "module.py {'id': 'chatcmpl-8Sg4y5NWPacghr2jm285txVNYkAhh', 'object': 'chat.completion', 'created': 1701845628, 'model': 'gpt-4-32k', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': '```python\\n## module.py\\n\\nclass Module:\\n    def __init__(self):\\n        self._parameters = {}\\n\\n    def forward(self, *input):\\n        raise NotImplementedError\\n\\n    def __call__(self, *input):\\n        return self.forward(*input)\\n\\n    def parameters(self):\\n        for name, param in self._parameters.items():\\n            yield param\\n\\n    def zero_grad(self):\\n        for param in self.parameters():\\n            param.zero_grad()\\n\\n    def add_parameter(self, name: str, param):\\n        if not isinstance(name, str):\\n            raise TypeError(\"parameter name should be a string. \"\\n                            \"Got {}\".format(type(name)))\\n        elif \\'.\\' in name:\\n            raise KeyError(\"parameter name can\\'t contain \\\\\".\\\\\"\")\\n        elif name == \\'\\':\\n            raise KeyError(\"parameter name can\\'t be empty string \\\\\"\\\\\"\")\\n        elif hasattr(self, name) and name not in self._parameters:\\n            raise KeyError(\"attribute \\'{}\\' already exists\".format(name))\\n        else:\\n            self._parameters[name] = param\\n\\nfrom tensor import Tensor\\nimport numpy as np\\n\\nclass Linear(Module):\\n    def __init__(self, in_features: int, out_features: int):\\n        super(Linear, self).__init__()\\n        self.in_features = in_features\\n        self.out_features = out_features\\n\\n        # weights and bias are the learnable parameters\\n        self.weight = Tensor(np.random.randn(out_features, in_features), requires_grad=True)\\n        self.bias = Tensor(np.random.randn(out_features), requires_grad=True)\\n\\n        self.add_parameter(\\'weight\\', self.weight)\\n        self.add_parameter(\\'bias\\', self.bias)\\n\\n    def forward(self, input):\\n        # Applies the linear transformation to the input data\\n        # Use numpy\\'s dot function for compatibility with Python 3.5 and below\\n        return input.dot(self.weight.T()) + self.bias\\n```'}, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 6487, 'completion_tokens': 376, 'total_tokens': 6863}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.py {'id': 'chatcmpl-8Sg5IumgmhWAbsCKepJIo9grL0z4Y', 'object': 'chat.completion', 'created': 1701845648, 'model': 'gpt-4-32k', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': '```python\\n## layers.py\\n\\nimport numpy as np\\nfrom module import Module\\nfrom tensor import Tensor\\nfrom autograd import Variable, Function\\n\\nclass Linear(Module):\\n    def __init__(self, in_features: int, out_features: int):\\n        super().__init__()\\n        self.in_features = in_features\\n        self.out_features = out_features\\n\\n        # Initialize weights and bias\\n        self.weight = Tensor(np.random.randn(in_features, out_features), requires_grad=True)\\n        self.bias = Tensor(np.random.randn(out_features), requires_grad=True)\\n\\n    def forward(self, input: Tensor) -> Tensor:\\n        return input.dot(self.weight) + self.bias\\n\\nclass ReLU(Module):\\n    def __init__(self):\\n        super().__init__()\\n\\n    def forward(self, input: Tensor) -> Tensor:\\n        return ReLUFunction.apply(input)\\n\\nclass ReLUFunction(Function):\\n    @staticmethod\\n    def forward(ctx, input: Variable) -> Variable:\\n        ctx.save_for_backward(input)\\n        return input.clamp_min(0)\\n\\n    @staticmethod\\n    def backward(ctx, grad_output: Variable) -> Variable:\\n        input, = ctx.saved_tensors\\n        grad_input = grad_output.clone()\\n        grad_input[input.data < 0] = 0\\n        return grad_input\\n\\nclass Sigmoid(Module):\\n    def __init__(self):\\n        super().__init__()\\n\\n    def forward(self, input: Tensor) -> Tensor:\\n        return SigmoidFunction.apply(input)\\n\\nclass SigmoidFunction(Function):\\n    @staticmethod\\n    def forward(ctx, input: Variable) -> Variable:\\n        output = 1 / (1 + np.exp(-input.data))\\n        ctx.save_for_backward(output)\\n        return Variable(output, requires_grad=input.requires_grad)\\n\\n    @staticmethod\\n    def backward(ctx, grad_output: Variable) -> Variable:\\n        output, = ctx.saved_tensors\\n        return grad_output * output * (1 - output)\\n\\nclass Tanh(Module):\\n    def __init__(self):\\n        super().__init__()\\n\\n    def forward(self, input: Tensor) -> Tensor:\\n        return TanhFunction.apply(input)\\n\\nclass TanhFunction(Function):\\n    @staticmethod\\n    def forward(ctx, input: Variable) -> Variable:\\n        output = np.tanh(input.data)\\n        ctx.save_for_backward(output)\\n        return Variable(output, requires_grad=input.requires_grad)\\n\\n    @staticmethod\\n    def backward(ctx, grad_output: Variable) -> Variable:\\n        output, = ctx.saved_tensors\\n        return grad_output * (1 - output ** 2)\\n```'}, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 6487, 'completion_tokens': 515, 'total_tokens': 7002}}\n",
      "optim.py {'id': 'chatcmpl-8Sg5fYE1RE7NnwPFzF4NhUVy5JrYe', 'object': 'chat.completion', 'created': 1701845671, 'model': 'gpt-4-32k', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': '## Code: Optimizer\\n\\n```python\\nfrom typing import List\\n\\nclass Tensor:\\n    def __init__(self, data: np.ndarray, requires_grad: bool = False):\\n        self.data = data\\n        self.requires_grad = requires_grad\\n        if requires_grad:\\n            self.grad = np.zeros_like(data)\\n        else:\\n            self.grad = None\\n\\n    def zero_grad(self):\\n        if self.grad is not None:\\n            self.grad.fill(0)\\n\\nclass Optimizer:\\n    def __init__(self, params: List[Tensor], lr: float = 0.001):\\n        self.params = params\\n        self.lr = lr\\n\\n    def step(self):\\n        for param in self.params:\\n            if param.requires_grad:\\n                if param.grad is not None:\\n                    param.data -= self.lr * param.grad\\n                else:\\n                    raise ValueError(\"Gradient is None for a parameter that requires gradient\")\\n\\n    def zero_grad(self):\\n        for param in self.params:\\n            param.zero_grad()\\n```\\nFormat example:\\n\\n```python\\nparams = [Tensor(np.array([1.0, 2.0, 3.0]), requires_grad=True), Tensor(np.array([4.0, 5.0, 6.0]), requires_grad=True)]\\noptimizer = Optimizer(params, lr=0.01)\\nfor param in params:\\n    param.grad = np.array([0.1, 0.1, 0.1])\\noptimizer.step()\\nfor param in params:\\n    print(param.data)  # prints: [0.99 1.99 2.99] and [3.99 4.99 5.99]\\noptimizer.zero_grad()\\nfor param in params:\\n    print(param.grad)  # prints: [0. 0. 0.] and [0. 0. 0.]\\n```'}, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 6487, 'completion_tokens': 379, 'total_tokens': 6866}}\n",
      "utils.py {'id': 'chatcmpl-8Sg5zFGUCYmiKa3mPmW3Wem0LwAdS', 'object': 'chat.completion', 'created': 1701845691, 'model': 'gpt-4-32k', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': '```python\\n## utils.py\\n\\nimport pickle\\nfrom typing import Any, List, Tuple\\nfrom module import Module\\n\\ndef save_model(model: Module, file_path: str) -> None:\\n    \"\"\"\\n    Saves the model to a file\\n    :param model: The model to be saved\\n    :param file_path: The path of the file\\n    :return: None\\n    \"\"\"\\n    if not isinstance(model, Module):\\n        raise TypeError(f\"model must be an instance of Module, got {type(model)} instead\")\\n    if not isinstance(file_path, str):\\n        raise TypeError(f\"file_path must be a string, got {type(file_path)} instead\")\\n\\n    with open(file_path, \\'wb\\') as f:\\n        pickle.dump(model, f)\\n\\ndef load_model(file_path: str) -> Module:\\n    \"\"\"\\n    Loads a model from a file\\n    :param file_path: The path of the file\\n    :return: The loaded model\\n    \"\"\"\\n    if not isinstance(file_path, str):\\n        raise TypeError(f\"file_path must be a string, got {type(file_path)} instead\")\\n\\n    with open(file_path, \\'rb\\') as f:\\n        model = pickle.load(f)\\n    return model\\n\\ndef split_dataset(dataset: List[Tuple[Any, Any]], ratio: float=0.8) -> Tuple[List[Tuple[Any, Any]], List[Tuple[Any, Any]]]:\\n    \"\"\"\\n    Splits a dataset into training and testing sets\\n    :param dataset: The dataset to be split\\n    :param ratio: The ratio of the training set to the whole dataset\\n    :return: A tuple (training_set, testing_set)\\n    \"\"\"\\n    if not isinstance(dataset, list):\\n        raise TypeError(f\"dataset must be a list, got {type(dataset)} instead\")\\n    if not 0 <= ratio <= 1:\\n        raise ValueError(f\"ratio must be between 0 and 1, got {ratio} instead\")\\n\\n    train_size = int(len(dataset) * ratio)\\n    train_set = dataset[:train_size]\\n    test_set = dataset[train_size:]\\n    return train_set, test_set\\n\\ndef normalize_data(data: List[float]) -> List[float]:\\n    \"\"\"\\n    Normalizes data to have zero mean and unit variance\\n    :param data: The data to be normalized\\n    :return: The normalized data\\n    \"\"\"\\n    if not isinstance(data, list):\\n        raise TypeError(f\"data must be a list, got {type(data)} instead\")\\n    if len(data) == 0:\\n        raise ValueError(\"data must not be an empty list\")\\n\\n    mean = sum(data) / len(data)\\n    variance = sum((x - mean) ** 2 for x in data) / len(data)\\n    if variance == 0:\\n        raise ValueError(\"variance of the data must not be zero\")\\n\\n    std_dev = variance ** 0.5\\n    return [(x - mean) / std_dev for x in data]\\n```'}, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 6487, 'completion_tokens': 609, 'total_tokens': 7096}}\n",
      "dataloader.py {'id': 'chatcmpl-8Sg6OWXiSDGPPPk6E95IoGRZerxZt', 'object': 'chat.completion', 'created': 1701845716, 'model': 'gpt-4-32k', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': '```python\\n## dataloader.py\\n\\nimport numpy as np\\nfrom typing import List, Tuple, Iterable\\nfrom itertools import islice\\nfrom tensor import Tensor\\n\\nclass DataLoader:\\n    def __init__(self, dataset: Iterable[Tuple[np.array, int]], batch_size: int=1, shuffle: bool=False):\\n        \"\"\"\\n        Initializes the dataloader with a dataset, batch size, and shuffle option\\n\\n        Args:\\n            dataset: Iterable of tuples, where each tuple contains a numpy array (data point) and an integer (label)\\n            batch_size: Number of samples per batch\\n            shuffle: Whether to shuffle the dataset before creating batches\\n        \"\"\"\\n        self.dataset = list(dataset)  # Convert dataset to list to make it indexable\\n        self.batch_size = batch_size\\n        self.shuffle = shuffle\\n\\n    def __len__(self):\\n        \"\"\"\\n        Returns the number of batches in the DataLoader\\n        \"\"\"\\n        return (len(self.dataset) + self.batch_size - 1) // self.batch_size  # rounds up\\n\\n    def __iter__(self):\\n        \"\"\"\\n        Provides an iterator to load data in batches\\n\\n        Yields:\\n            Batches of data points and labels\\n        \"\"\"\\n        # If shuffle is enabled, shuffle the dataset before creating batches\\n        if self.shuffle:\\n            np.random.shuffle(self.dataset)\\n\\n        # Yield batches as Tensors\\n        for i in range(0, len(self.dataset), self.batch_size):\\n            data_batch, labels_batch = zip(*islice(self.dataset, i, i + self.batch_size))\\n            yield Tensor(np.array(data_batch)), Tensor(np.array(labels_batch))\\n```\\nFormat Example:\\n```python\\n# create a dataset\\ndataset = [(np.array([1,2,3]), 0), (np.array([4,5,6]), 1), (np.array([7,8,9]), 0)]\\n# create a dataloader\\nloader = DataLoader(dataset, batch_size=2, shuffle=True)\\n# load data in batches\\nfor data, labels in loader:\\n    print(data, labels)\\n```'}, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 6490, 'completion_tokens': 428, 'total_tokens': 6918}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main.py {'id': 'chatcmpl-8Sg6iw8pABcsbpf4R2OTpvYGVaNeY', 'object': 'chat.completion', 'created': 1701845736, 'model': 'gpt-4-32k', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': '```python\\n## Revised main.py\\n\\nimport torch\\nfrom torch import nn, optim\\nfrom torch.utils.data import DataLoader\\nfrom torchvision import datasets, transforms\\n\\nclass MLP(nn.Module):\\n    def __init__(self):\\n        super(MLP, self).__init__()\\n        self.model = nn.Sequential(\\n            nn.Linear(784, 200),\\n            nn.LeakyReLU(inplace=True),\\n            nn.Linear(200, 200),\\n            nn.LeakyReLU(inplace=True),\\n            nn.Linear(200, 10),\\n            nn.LeakyReLU(inplace=True),\\n        )\\n\\n    def forward(self, x):\\n        return self.model(x)\\n\\ndef load_data():\\n    train_loader = DataLoader(\\n        datasets.MNIST(\\'./data\\', train=True, download=True,\\n                       transform=transforms.Compose([\\n                           transforms.ToTensor(),\\n                           transforms.Normalize((0.1307,), (0.3081,))\\n                       ])),\\n        batch_size=batch_size, shuffle=True)\\n\\n    test_loader = DataLoader(\\n        datasets.MNIST(\\'./data\\', train=False, transform=transforms.Compose([\\n            transforms.ToTensor(),\\n            transforms.Normalize((0.1307,), (0.3081,))\\n        ])),\\n        batch_size=batch_size, shuffle=True)\\n    \\n    return train_loader, test_loader\\n\\ndef train_model(device, net, optimizer, criteon, train_loader, epoch):\\n    for batch_idx, (data, target) in enumerate(train_loader):\\n        data = data.view(-1, 28*28)\\n        data, target = data.to(device), target.to(device)\\n\\n        logits = net(data)\\n        loss = criteon(logits, target)\\n\\n        optimizer.zero_grad()\\n        loss.backward()\\n        optimizer.step()\\n\\n        if batch_idx % 100 == 0:\\n            print(\\'Train Epoch: {} [{}/{} ({:.0f}%)]\\\\tLoss: {:.6f}\\'.format(\\n                epoch, batch_idx * len(data), len(train_loader.dataset),\\n                       100. * batch_idx / len(train_loader), loss.item()))\\n\\ndef test_model(device, net, criteon, test_loader):\\n    test_loss = 0\\n    correct = 0\\n    for data, target in test_loader:\\n        data = data.view(-1, 28 * 28)\\n        data, target = data.to(device), target.to(device)\\n        logits = net(data)\\n        test_loss += criteon(logits, target).item()\\n\\n        pred = logits.data.max(1)[1]\\n        correct += pred.eq(target.data).sum()\\n\\n    test_loss /= len(test_loader.dataset)\\n    print(\\'\\\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\\\n\\'.format(\\n        test_loss, correct, len(test_loader.dataset),\\n        100. * correct / len(test_loader.dataset)))\\n\\n\\ndef main():\\n    batch_size = 200\\n    learning_rate = 0.01\\n    epochs = 10\\n\\n    # Load Data\\n    train_loader, test_loader = load_data()\\n\\n    # Check if cuda is available\\n    device = torch.device(\\'cuda:0\\' if torch.cuda.is_available() else \\'cpu\\')\\n\\n    net = MLP().to(device)\\n    optimizer = optim.SGD(net.parameters(), lr=learning_rate)\\n    criteon = nn.CrossEntropyLoss().to(device)\\n\\n    for epoch in range(epochs):\\n        # Train Model\\n        train_model(device, net, optimizer, criteon, train_loader, epoch)\\n        # Test Model\\n        test_model(device, net, criteon, test_loader)\\n\\nif __name__ == \"__main__\":\\n    main()\\n```'}, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 6487, 'completion_tokens': 735, 'total_tokens': 7222}}\n"
     ]
    }
   ],
   "source": [
    "reviews_r2 = {}\n",
    "for file in file_apis:\n",
    "    reviews_r2[file] = r2_responses[file]['choices'][0]['message']['content']\n",
    "r3_queries, r3_responses = round3(file_codes_r1, reviews_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b9198192",
   "metadata": {},
   "outputs": [],
   "source": [
    "log = [{'r1_queries': r1_queries, 'r1_responses': r1_responses}, \n",
    "       {'r2_queries': r2_queries, 'r2_responses': r2_responses}, \n",
    "       {'r3_queries': r3_queries, 'r3_responses': r3_responses}]\n",
    "\n",
    "# Open the file in write mode  \n",
    "with open('log_3_round.json', 'w') as file:  \n",
    "    # Iterate over the list of dictionaries  \n",
    "    for d in log:  \n",
    "        # Convert the dictionary to a JSON string  \n",
    "        json_str = json.dumps(d)  \n",
    "        # Write the JSON string to the file with a newline  \n",
    "        file.write(json_str + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd247887",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
