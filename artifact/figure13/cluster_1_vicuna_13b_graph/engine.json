{
    "engine_name": "vicuna-13b-v1.3_local",
    "model": "lmsys/vicuna-13b-v1.3",
    "host": "localhost",
    "port": 9001,
    "engine_type": "builtin",
    "random_seed": 0,
    "tokenizer": "hf-internal-testing/llama-tokenizer",
    "fill_chunk_size": -1,
    "tasks_capacity": 256,
    "tokens_capacity": 60000,
    "instance": {
        "block_size": 16,
        "num_kv_cache_blocks": 4000,
        "attn_func": "xformers_fill_vllm_paged_attention_generate",
        "max_seq_len": 65536
    },
    "scheduler": {
        "max_batch_size": 256,
        "max_num_batched_tokens": 12288,
        "max_total_tokens": 45000,
        "policy": "fifo"
    },
    "serve_core": {
        "host": "localhost",
        "port": 9000
    }
}